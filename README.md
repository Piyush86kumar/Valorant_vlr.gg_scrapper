# VLR.gg Event Scraper

A modular scraping system for VLR.gg tournament events with SQLite database integration.

## ğŸš€ Quick Start

1. **Install dependencies**:
```bash
pip install streamlit requests beautifulsoup4 pandas plotly
```

2. **Run the application**:
```bash
streamlit run vlr_streamlit_ui.py
```

3. **Use the scraper**:
   - Enter a VLR.gg event URL (e.g., `https://www.vlr.gg/event/2097/valorant-champions-2024`)
   - Select what to scrape (Matches, Player Stats, Maps & Agents)
   - Review the complete scraped data
   - Save to database or download files

## ğŸ“ Project Structure

```
â”œâ”€â”€ matches_scraper.py          # Scrapes match data and results
â”œâ”€â”€ player_stats_scraper.py     # Scrapes player statistics
â”œâ”€â”€ maps_agents_scraper.py      # Scrapes agent usage and map data
â”œâ”€â”€ vlr_scraper_coordinator.py  # Coordinates all scrapers
â”œâ”€â”€ vlr_database.py             # SQLite database operations
â”œâ”€â”€ vlr_streamlit_ui.py         # Streamlit web interface
â””â”€â”€ README.md                   # This file
```

## ğŸ¯ Features

### **Modular Scraping**
- **ğŸ† Matches Scraper**: Match details, scores, team performance
- **ğŸ“Š Player Stats Scraper**: Individual player statistics and rankings
- **ğŸ­ Maps & Agents Scraper**: Agent usage, map picks, meta analysis

### **Data Management**
- **ğŸ—„ï¸ SQLite Database**: Persistent storage for historical analysis
- **ğŸ“¥ File Downloads**: JSON and CSV export options
- **ğŸ‘ï¸ Data Preview**: Complete data review before saving

### **User Interface**
- **Simple 3-step workflow**: Scrape â†’ Review â†’ Save
- **Real-time progress**: Live scraping status updates
- **Complete data display**: Full tables showing all scraped data

## ğŸ”§ Usage Examples

### **Using Individual Scrapers**
```python
from matches_scraper import MatchesScraper

scraper = MatchesScraper()
matches_data = scraper.scrape_matches("https://www.vlr.gg/event/2097/valorant-champions-2024")
print(f"Found {matches_data['total_matches']} matches")
```

### **Using the Coordinator**
```python
from vlr_scraper_coordinator import VLRScraperCoordinator

coordinator = VLRScraperCoordinator()
data = coordinator.scrape_comprehensive(
    "https://www.vlr.gg/event/2097/valorant-champions-2024",
    scrape_matches=True,
    scrape_stats=True,
    scrape_maps_agents=True
)
```

### **Using the Database**
```python
from vlr_database import VLRDatabase

db = VLRDatabase()
event_id = db.save_comprehensive_data(scraped_data)
events = db.get_events_list()
```

## ğŸ“Š Data Structure

### **Matches Data**
- Team names and scores
- Match status (Completed/Scheduled)
- Stage and tournament information
- Series details

### **Player Statistics**
- Individual performance metrics (ACS, K/D, ADR, etc.)
- Team affiliations
- Rankings and comparisons

### **Maps & Agents Data**
- Agent usage percentages
- Win rates by agent
- Map pick rates
- Meta analysis with role categorization

## ğŸ› ï¸ Requirements

- Python 3.7+
- streamlit
- requests
- beautifulsoup4
- pandas
- plotly

## ğŸ“ Example URLs

- **Valorant Champions 2024**: `https://www.vlr.gg/event/2097/valorant-champions-2024`
- **Masters Madrid 2024**: `https://www.vlr.gg/event/1921/champions-tour-2024-masters-madrid`
- **Masters Shanghai 2024**: `https://www.vlr.gg/event/1999/champions-tour-2024-masters-shanghai`

## ğŸ® Workflow

1. **Step 1 - Scrape**: Enter URL and select data types to scrape
2. **Step 2 - Review**: View complete scraped data in tables
3. **Step 3 - Save**: Choose database storage or file download

## ğŸ”„ Database Features

- **Persistent Storage**: Save events for historical analysis
- **Easy Retrieval**: Browse and view previously scraped events
- **Export Options**: Export data to CSV files
- **Data Management**: View, delete, and organize saved events

## ğŸ“ˆ Future Enhancements

- Support for additional VLR.gg data types
- Advanced data visualization
- Automated scheduling for regular scraping
- API endpoint creation

---

**Note**: This tool is for educational and research purposes. Please respect VLR.gg's terms of service and use responsibly.